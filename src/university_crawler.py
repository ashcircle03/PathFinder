"""
ëŒ€í•™ ì…í•™ì²˜ í™ˆí˜ì´ì§€ í¬ë¡¤ëŸ¬
ì‹¤ì œ ëŒ€í•™ í™ˆí˜ì´ì§€ì—ì„œ í•™ê³¼ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
import json
import re
from datetime import datetime


class UniversityCrawler:
    """ëŒ€í•™ ì…í•™ì²˜ ì •ë³´ í¬ë¡¤ëŸ¬"""

    def __init__(self):
        self.universities = {
            "ì„œìš¸ëŒ€í•™êµ": {
                "url": "https://admission.snu.ac.kr",
                "departments_url": "https://www.snu.ac.kr/academics/undergraduate",
            },
            "ì—°ì„¸ëŒ€í•™êµ": {
                "url": "https://admission.yonsei.ac.kr",
                "departments_url": "https://www.yonsei.ac.kr/sc/support/departments.jsp",
            },
            "ê³ ë ¤ëŒ€í•™êµ": {
                "url": "https://admission.korea.ac.kr",
                "departments_url": "https://www.korea.ac.kr/mbshome/mbs/university/subview.do?id=university_010401000000",
            },
        }
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        self.crawled_data = []

    async def fetch_page(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        """ë¹„ë™ê¸°ë¡œ í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            async with session.get(url, headers=self.headers, timeout=10) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    print(f"âš ï¸ {url} ì ‘ê·¼ ì‹¤íŒ¨: HTTP {response.status}")
                    return None
        except Exception as e:
            print(f"âŒ {url} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return None

    def parse_department_info(self, html: str, university_name: str) -> List[Dict[str, Any]]:
        """HTMLì—ì„œ í•™ê³¼ ì •ë³´ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
        soup = BeautifulSoup(html, "html.parser")
        departments = []

        # ì¼ë°˜ì ì¸ í•™ê³¼ ì •ë³´ ì¶”ì¶œ íŒ¨í„´
        # ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ì¶° ìˆ˜ì • í•„ìš”
        dept_elements = soup.find_all(["div", "li", "tr"], class_=re.compile(r"(dept|department|major|college)"))

        for element in dept_elements:
            dept_info = self._extract_department_details(element, university_name)
            if dept_info:
                departments.append(dept_info)

        return departments

    def _extract_department_details(self, element, university_name: str) -> Optional[Dict[str, Any]]:
        """ê°œë³„ í•™ê³¼ ìš”ì†Œì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            # í•™ê³¼ëª… ì¶”ì¶œ
            name = element.find(["h3", "h4", "strong", "b"])
            if not name:
                return None

            dept_name = name.get_text(strip=True)

            # ì„¤ëª… ì¶”ì¶œ
            description = element.find(["p", "div"], class_=re.compile(r"(desc|description|intro)"))
            desc_text = description.get_text(strip=True) if description else ""

            # ê¸°ë³¸ ì •ë³´ êµ¬ì„±
            return {
                "university": university_name,
                "name": dept_name,
                "description": desc_text,
                "url": element.find("a")["href"] if element.find("a") else "",
                "crawled_at": datetime.now().isoformat(),
            }
        except Exception as e:
            print(f"âš ï¸ í•™ê³¼ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    async def crawl_university(self, university_name: str, urls: Dict[str, str]) -> List[Dict[str, Any]]:
        """íŠ¹ì • ëŒ€í•™ì˜ í•™ê³¼ ì •ë³´ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        print(f"ğŸ” {university_name} í¬ë¡¤ë§ ì‹œì‘...")

        async with aiohttp.ClientSession() as session:
            # í•™ê³¼ ëª©ë¡ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            html = await self.fetch_page(session, urls["departments_url"])

            if html:
                departments = self.parse_department_info(html, university_name)
                print(f"âœ… {university_name}: {len(departments)}ê°œ í•™ê³¼ ë°œê²¬")
                return departments
            else:
                print(f"âŒ {university_name} í¬ë¡¤ë§ ì‹¤íŒ¨")
                return []

    async def crawl_all_universities(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ëŒ€í•™ì˜ í•™ê³¼ ì •ë³´ë¥¼ ë³‘ë ¬ë¡œ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        print("ğŸš€ ëŒ€í•™ ì…í•™ì²˜ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘...")

        tasks = [
            self.crawl_university(name, urls)
            for name, urls in self.universities.items()
        ]

        results = await asyncio.gather(*tasks)

        # ê²°ê³¼ ë³‘í•©
        all_departments = []
        for dept_list in results:
            all_departments.extend(dept_list)

        self.crawled_data = all_departments
        print(f"âœ… ì´ {len(all_departments)}ê°œ í•™ê³¼ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")

        return all_departments

    def save_to_json(self, filepath: str = "data/university_departments.json"):
        """í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(self.crawled_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
        except Exception as e:
            print(f"[ERROR] ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

    async def crawl_specific_department_details(
        self, department_url: str, university_name: str
    ) -> Dict[str, Any]:
        """íŠ¹ì • í•™ê³¼ì˜ ìƒì„¸ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        async with aiohttp.ClientSession() as session:
            html = await self.fetch_page(session, department_url)

            if not html:
                return {}

            soup = BeautifulSoup(html, "html.parser")

            details = {
                "university": university_name,
                "curriculum": [],
                "admission_info": {},
                "career_prospects": [],
            }

            # ì»¤ë¦¬í˜ëŸ¼ ì •ë³´ ì¶”ì¶œ
            curriculum_section = soup.find(["div", "section"], class_=re.compile(r"(curriculum|course)"))
            if curriculum_section:
                courses = curriculum_section.find_all(["li", "div"])
                details["curriculum"] = [course.get_text(strip=True) for course in courses[:20]]

            # ì…í•™ ì •ë³´ ì¶”ì¶œ
            admission_section = soup.find(["div", "section"], class_=re.compile(r"(admission|entrance)"))
            if admission_section:
                details["admission_info"] = {
                    "content": admission_section.get_text(strip=True)[:500]
                }

            # ì§„ë¡œ ì •ë³´ ì¶”ì¶œ
            career_section = soup.find(["div", "section"], class_=re.compile(r"(career|job|future)"))
            if career_section:
                careers = career_section.find_all(["li", "p"])
                details["career_prospects"] = [career.get_text(strip=True) for career in careers[:10]]

            return details


class MockUniversityCrawler:
    """
    ì‹¤ì œ í¬ë¡¤ë§ ëŒ€ì‹  ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” Mock í¬ë¡¤ëŸ¬
    (ëŒ€í•™ ì›¹ì‚¬ì´íŠ¸ ë¶€í•˜ ë°©ì§€ ë° ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
    """

    def __init__(self):
        self.universities = [
            "ì„œìš¸ëŒ€í•™êµ", "ì—°ì„¸ëŒ€í•™êµ", "ê³ ë ¤ëŒ€í•™êµ", "ì¹´ì´ìŠ¤íŠ¸", "í¬ìŠ¤í…",
            "ì„±ê· ê´€ëŒ€í•™êµ", "í•œì–‘ëŒ€í•™êµ", "ì¤‘ì•™ëŒ€í•™êµ", "ê²½í¬ëŒ€í•™êµ", "ì´í™”ì—¬ìëŒ€í•™êµ"
        ]

    async def crawl_all_universities(self) -> List[Dict[str, Any]]:
        """ìƒ˜í”Œ ëŒ€í•™ í•™ê³¼ ì •ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        print("[Mock Crawler] ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")

        sample_departments = []

        # ê¸°ì¡´ majors.jsonì˜ í•™ê³¼ë“¤ì„ ê° ëŒ€í•™ì— ë§¤í•‘
        base_majors = [
            {
                "name": "ì»´í“¨í„°ê³µí•™ê³¼",
                "category": "ê³µí•™",
                "description_template": "{univ} ì»´í“¨í„°ê³µí•™ê³¼ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ì™€ í•˜ë“œì›¨ì–´ë¥¼ ì•„ìš°ë¥´ëŠ” ì¢…í•©ì ì¸ ì»´í“¨í„° ê³¼í•™ êµìœ¡ì„ ì œê³µí•©ë‹ˆë‹¤. AI, ë¹…ë°ì´í„°, ì‚¬ì´ë²„ë³´ì•ˆ ë“± ìµœì‹  ê¸°ìˆ ì„ ë°°ìš°ë©°, ì‚°í•™í˜‘ë ¥ í”„ë¡œê·¸ë¨ì„ í†µí•´ ì‹¤ë¬´ ê²½í—˜ì„ ìŒ“ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "keywords": ["í”„ë¡œê·¸ë˜ë°", "AI", "ë¹…ë°ì´í„°", "ì†Œí”„íŠ¸ì›¨ì–´", "ì•Œê³ ë¦¬ì¦˜"],
                "admission_quota": "60-100ëª…",
                "graduation_requirements": "ì „ê³µ 60í•™ì  ì´ìƒ, ì¡¸ì—…ë…¼ë¬¸ ë˜ëŠ” ìº¡ìŠ¤í†¤ í”„ë¡œì íŠ¸",
            },
            {
                "name": "ê²½ì˜í•™ê³¼",
                "category": "ìƒê²½",
                "description_template": "{univ} ê²½ì˜í•™ê³¼ëŠ” ê²½ì˜ ì „ë°˜ì˜ ì´ë¡ ê³¼ ì‹¤ë¬´ë¥¼ ê²¸ë¹„í•œ ê¸€ë¡œë²Œ ë¦¬ë”ë¥¼ ì–‘ì„±í•©ë‹ˆë‹¤. ë§ˆì¼€íŒ…, ì¬ë¬´, ì¸ì‚¬ì¡°ì§, ê²½ì˜ì „ëµ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ë¥¼ í•™ìŠµí•˜ë©°, êµ­ë‚´ì™¸ ê¸°ì—…ê³¼ì˜ ì¸í„´ì‹­ ê¸°íšŒë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
                "keywords": ["ê²½ì˜", "ë§ˆì¼€íŒ…", "ì¬ë¬´", "ì „ëµ", "ë¦¬ë”ì‹­"],
                "admission_quota": "80-120ëª…",
                "graduation_requirements": "ì „ê³µ 54í•™ì  ì´ìƒ, ê²½ì˜í•™ ì¸ì¦ í”„ë¡œê·¸ë¨ ì´ìˆ˜",
            },
            {
                "name": "ì¸ê³µì§€ëŠ¥í•™ê³¼",
                "category": "ê³µí•™",
                "description_template": "{univ} ì¸ê³µì§€ëŠ¥í•™ê³¼ëŠ” ë¯¸ë˜ AI ê¸°ìˆ ì„ ì„ ë„í•  ì „ë¬¸ ì¸ë ¥ì„ ì–‘ì„±í•©ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹, ìì—°ì–´ì²˜ë¦¬, ì»´í“¨í„°ë¹„ì „ ë“±ì„ ì§‘ì¤‘ì ìœ¼ë¡œ í•™ìŠµí•˜ë©°, ìµœì‹  GPU ì„œë²„ì™€ ì—°êµ¬ í™˜ê²½ì„ ì œê³µí•©ë‹ˆë‹¤.",
                "keywords": ["AI", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ë°ì´í„°ê³¼í•™", "ë¹…ë°ì´í„°"],
                "admission_quota": "40-60ëª…",
                "graduation_requirements": "ì „ê³µ 66í•™ì  ì´ìƒ, AI í”„ë¡œì íŠ¸ í•„ìˆ˜",
            },
            {
                "name": "ì‹¬ë¦¬í•™ê³¼",
                "category": "ì¸ë¬¸ì‚¬íšŒ",
                "description_template": "{univ} ì‹¬ë¦¬í•™ê³¼ëŠ” ì¸ê°„ì˜ ë§ˆìŒê³¼ í–‰ë™ì„ ê³¼í•™ì ìœ¼ë¡œ íƒêµ¬í•©ë‹ˆë‹¤. ì„ìƒì‹¬ë¦¬, ìƒë‹´ì‹¬ë¦¬, ì‚°ì—…ì‹¬ë¦¬ ë“± ë‹¤ì–‘í•œ ì„¸ë¶€ ì „ê³µì„ ì œê³µí•˜ë©°, ì‹¬ë¦¬ ì‹¤í—˜ì‹¤ê³¼ ìƒë‹´ ì„¼í„°ì—ì„œ ì‹¤ìŠµ ê²½í—˜ì„ ìŒ“ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "keywords": ["ì‹¬ë¦¬", "ìƒë‹´", "ì¸ê°„í–‰ë™", "ì •ì‹ ê±´ê°•", "ë‡Œê³¼í•™"],
                "admission_quota": "50-70ëª…",
                "graduation_requirements": "ì „ê³µ 48í•™ì  ì´ìƒ, ì‹¬ë¦¬í•™ ì‹¤í—˜ ì‹¤ìŠµ ì´ìˆ˜",
            },
            {
                "name": "ì „ìì „ê¸°ê³µí•™ê³¼",
                "category": "ê³µí•™",
                "description_template": "{univ} ì „ìì „ê¸°ê³µí•™ê³¼ëŠ” ë°˜ë„ì²´, í†µì‹ , ë¡œë´‡, ì—ë„ˆì§€ ë¶„ì•¼ì˜ í•µì‹¬ ê¸°ìˆ ì„ ë‹¤ë£¹ë‹ˆë‹¤. ì²¨ë‹¨ ì‹¤í—˜ ì¥ë¹„ë¥¼ í™œìš©í•œ ì‹¤ìŠµê³¼ ëŒ€ê¸°ì—… ì—°êµ¬ì†Œì™€ì˜ ì‚°í•™í˜‘ë ¥ì„ í†µí•´ ì‹¤ë¬´í˜• ì¸ì¬ë¥¼ ì–‘ì„±í•©ë‹ˆë‹¤.",
                "keywords": ["ì „ì", "ë°˜ë„ì²´", "í†µì‹ ", "íšŒë¡œ", "ì„ë² ë””ë“œ"],
                "admission_quota": "70-90ëª…",
                "graduation_requirements": "ì „ê³µ 60í•™ì  ì´ìƒ, ì¢…í•©ì„¤ê³„ í”„ë¡œì íŠ¸",
            },
        ]

        for univ in self.universities:
            for major in base_majors:
                dept_info = {
                    "university": univ,
                    "name": major["name"],
                    "category": major["category"],
                    "description": major["description_template"].format(univ=univ),
                    "keywords": major["keywords"],
                    "admission_info": {
                        "quota": major["admission_quota"],
                        "requirements": major["graduation_requirements"],
                        "website": f"https://{univ.replace('ëŒ€í•™êµ', '')}.ac.kr/dept/{major['name'].replace('ê³¼', '')}",
                    },
                    "curriculum": [
                        "1í•™ë…„: ì „ê³µê¸°ì´ˆ ë° êµì–‘",
                        "2í•™ë…„: ì „ê³µí•„ìˆ˜ ê³¼ëª©",
                        "3í•™ë…„: ì „ê³µì‹¬í™” ë° ì„ íƒ",
                        "4í•™ë…„: ìº¡ìŠ¤í†¤ í”„ë¡œì íŠ¸ ë° ì¡¸ì—…ì—°êµ¬",
                    ],
                    "career_prospects": [
                        f"{major['category']} ê´€ë ¨ ëŒ€ê¸°ì—… ì·¨ì—…",
                        "ì—°êµ¬ì†Œ ë° êµ­ì±…ê¸°ê´€",
                        "ëŒ€í•™ì› ì§„í•™",
                        "ì°½ì—…",
                    ],
                    "crawled_at": datetime.now().isoformat(),
                    "source": "mock_data",
                }
                sample_departments.append(dept_info)

        print(f"[OK] {len(sample_departments)}ê°œ ìƒ˜í”Œ í•™ê³¼ ì •ë³´ ìƒì„± ì™„ë£Œ")
        return sample_departments

    def save_to_json(self, filepath: str = "data/university_departments.json"):
        """ìƒì„±í•œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        asyncio.run(self._save_async(filepath))

    async def _save_async(self, filepath: str):
        data = await self.crawl_all_universities()
        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"[SAVED] ìƒ˜í”Œ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
        except Exception as e:
            print(f"[ERROR] ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")


async def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    # Mock í¬ë¡¤ëŸ¬ ì‚¬ìš© (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
    crawler = MockUniversityCrawler()
    departments = await crawler.crawl_all_universities()

    # JSON ì €ì¥
    try:
        with open("data/university_departments.json", "w", encoding="utf-8") as f:
            json.dump(departments, f, ensure_ascii=False, indent=2)
        print(f"[SAVED] ë°ì´í„° ì €ì¥ ì™„ë£Œ: data/university_departments.json")
    except Exception as e:
        print(f"[ERROR] ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

    # í†µê³„ ì¶œë ¥
    universities = set(d["university"] for d in departments)
    print(f"\n[STATS] í¬ë¡¤ë§ í†µê³„:")
    print(f"   - ëŒ€í•™ ìˆ˜: {len(universities)}")
    print(f"   - í•™ê³¼ ìˆ˜: {len(departments)}")
    print(f"   - ëŒ€í•™ ëª©ë¡: {', '.join(universities)}")


if __name__ == "__main__":
    asyncio.run(main())
