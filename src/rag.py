"""
LangChain ê¸°ë°˜ RAG (Retrieval-Augmented Generation) í•™ê³¼ ì¶”ì²œ ì‹œìŠ¤í…œ
"""
import os
from typing import List, Dict, Any, Optional
from langchain_qdrant import QdrantVectorStore
from langchain_ollama import OllamaLLM
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.chains import RetrievalQA
from langchain.schema import Document
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field


class MajorRecommendation(BaseModel):
    """í•™ê³¼ ì¶”ì²œ ê²°ê³¼ ëª¨ë¸"""
    recommended_majors: List[str] = Field(description="ì¶”ì²œ í•™ê³¼ ëª©ë¡ (3-5ê°œ)")
    reasoning: str = Field(description="ì¶”ì²œ ì´ìœ  ë° ê° í•™ê³¼ì— ëŒ€í•œ ì„¤ëª…")


class MajorRecommendationRAG:
    """LangChain ê¸°ë°˜ í•™ê³¼ ì¶”ì²œ RAG ì‹œìŠ¤í…œ"""

    def __init__(self):
        # í™˜ê²½ ë³€ìˆ˜
        qdrant_host = os.getenv("QDRANT_HOST", "http://localhost:6333")
        ollama_host = os.getenv("OLLAMA_HOST", "http://localhost:11434")
        self.llm_model = os.getenv("OLLAMA_MODEL", "qwen2.5:32b")
        self.collection_name = "majors"

        print("ðŸš€ LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

        # 1. ìž„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (í•œêµ­ì–´ íŠ¹í™”)
        print("ðŸ“¦ ìž„ë² ë”© ëª¨ë¸ ë¡œë”©...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        # 2. Qdrant Vector Store ì´ˆê¸°í™”
        print("ðŸ—„ï¸ Qdrant Vector Store ì—°ê²°...")
        self.vectorstore = QdrantVectorStore.from_existing_collection(
            embedding=self.embeddings,
            collection_name=self.collection_name,
            url=qdrant_host,
        )

        # 3. LLM ì´ˆê¸°í™” (Ollama)
        print(f"ðŸ¤– LLM ì´ˆê¸°í™”: {self.llm_model}")
        self.llm = OllamaLLM(
            model=self.llm_model,
            base_url=ollama_host,
            temperature=0.7,
            # í•œêµ­ì–´ ì‘ë‹µ ê°œì„ ì„ ìœ„í•œ ì„¤ì •
            system="You are a Korean university counselor. Always respond in pure Korean (Hangul only). Never use Chinese characters (Hanja), Japanese, or English except for proper nouns.",
        )

        # 4. Output Parser ì„¤ì •
        self.output_parser = PydanticOutputParser(pydantic_object=MajorRecommendation)

        # 5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        self._setup_prompts()

        # 6. Retriever ì„¤ì •
        self.retriever = self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}  # ìƒìœ„ 5ê°œ í•™ê³¼ ê²€ìƒ‰
        )

        print("âœ… LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")

    def _setup_prompts(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •"""

        # RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.rag_template = """ë‹¹ì‹ ì€ í•œêµ­ì˜ ëŒ€í•™ ì§„í•™ ìƒë‹´ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
ì œê³µëœ í•™ê³¼ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, í•™ìƒì—ê²Œ ê°€ìž¥ ì í•©í•œ í•™ê³¼ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

[ê²€ìƒ‰ëœ í•™ê³¼ ì •ë³´]
{context}

[í•™ìƒ ê´€ì‹¬ì‚¬]
{question}

ìœ„ ê²€ìƒ‰ëœ í•™ê³¼ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ:
1. í•™ìƒì˜ ê´€ì‹¬ì‚¬ì™€ ê°€ìž¥ ìž˜ ë§žëŠ” í•™ê³¼ 3-5ê°œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”
2. ê° í•™ê³¼ë¥¼ ì¶”ì²œí•˜ëŠ” êµ¬ì²´ì ì¸ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”
3. ê° í•™ê³¼ì˜ íŠ¹ì§•ê³¼ ì§„ë¡œ ì „ë§ì„ ê°„ëžµížˆ ì†Œê°œí•´ì£¼ì„¸ìš”

ì¤‘ìš” ê·œì¹™:
- ë°˜ë“œì‹œ ê²€ìƒ‰ëœ í•™ê³¼ ì •ë³´ ë‚´ì—ì„œë§Œ ì¶”ì²œí•˜ì„¸ìš”
- ì¡´ìž¬í•˜ì§€ ì•ŠëŠ” í•™ê³¼ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”
- ìˆœìˆ˜ í•œê¸€ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš” (í•œìž ì‚¬ìš© ê¸ˆì§€)
- ì¤‘êµ­ì–´ë‚˜ ì¼ë³¸ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ëª¨ë“  ì „ë¬¸ ìš©ì–´ë„ í•œê¸€ë¡œ í‘œê¸°í•˜ì„¸ìš”

{format_instructions}

ë‹µë³€:"""

        self.prompt = PromptTemplate(
            template=self.rag_template,
            input_variables=["context", "question"],
            partial_variables={
                "format_instructions": self.output_parser.get_format_instructions()
            }
        )

    def search_similar_majors(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        ì‚¬ìš©ìžì˜ ê´€ì‹¬ì‚¬ì™€ ìœ ì‚¬í•œ í•™ê³¼ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.

        Args:
            query: ì‚¬ìš©ìžì˜ ê´€ì‹¬ì‚¬ ë¬¸ìžì—´
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜

        Returns:
            ìœ ì‚¬í•œ í•™ê³¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        # LangChain retrieverë¥¼ í†µí•œ ê²€ìƒ‰
        docs = self.retriever.get_relevant_documents(query)[:top_k]

        results = []
        for doc in docs:
            results.append({
                "score": doc.metadata.get("_score", 0.0),
                "major_name": doc.metadata.get("name", ""),
                "category": doc.metadata.get("category", ""),
                "description": doc.page_content,
                "keywords": doc.metadata.get("keywords", []),
                "career_paths": doc.metadata.get("career_paths", []),
                "related_subjects": doc.metadata.get("related_subjects", []),
                "skills_required": doc.metadata.get("skills_required", [])
            })

        return results

    def generate_recommendation(
        self,
        interests: str,
        search_results: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ëœ í•™ê³¼ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMì„ ì‚¬ìš©í•˜ì—¬ ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            interests: ì‚¬ìš©ìžì˜ ê´€ì‹¬ì‚¬
            search_results: ê²€ìƒ‰ëœ í•™ê³¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ìžë™ ê²€ìƒ‰)

        Returns:
            ì¶”ì²œ í•™ê³¼ì™€ ì´ìœ ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
        """
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ìžë™ ê²€ìƒ‰
        if search_results is None:
            search_results = self.search_similar_majors(interests)

        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        for idx, result in enumerate(search_results, 1):
            context_parts.append(
                f"{idx}. {result['major_name']} ({result['category']})\n"
                f"   ì„¤ëª…: {result['description']}\n"
                f"   í‚¤ì›Œë“œ: {', '.join(result['keywords'][:5])}\n"
                f"   ì§„ë¡œ: {', '.join(result['career_paths'][:3])}"
            )

        context = "\n\n".join(context_parts)

        try:
            # LLMì„ í†µí•œ ì¶”ì²œ ìƒì„±
            prompt_text = self.prompt.format(context=context, question=interests)
            response = self.llm.invoke(prompt_text)

            # Output Parserë¥¼ í†µí•œ íŒŒì‹± ì‹œë„
            try:
                parsed_response = self.output_parser.parse(response)
                return {
                    "recommended_majors": parsed_response.recommended_majors,
                    "reasoning": parsed_response.reasoning,
                    "retrieved_context": search_results
                }
            except Exception as parse_error:
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ fallback: ê²€ìƒ‰ëœ í•™ê³¼ëª… ì‚¬ìš©
                print(f"âš ï¸ Output íŒŒì‹± ì‹¤íŒ¨, fallback ì‚¬ìš©: {parse_error}")
                return {
                    "recommended_majors": [r['major_name'] for r in search_results[:5]],
                    "reasoning": response,
                    "retrieved_context": search_results
                }

        except Exception as e:
            # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜
            print(f"âŒ LLM ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "recommended_majors": [r['major_name'] for r in search_results[:5]],
                "reasoning": f"ê²€ìƒ‰ëœ í•™ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œí•©ë‹ˆë‹¤. (LLM ì˜¤ë¥˜: {str(e)})",
                "retrieved_context": search_results
            }

    def recommend_majors(self, interests: str, top_k: int = 5) -> Dict[str, Any]:
        """
        ì‚¬ìš©ìžì˜ ê´€ì‹¬ì‚¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ê³¼ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤. (í†µí•© ë©”ì„œë“œ)

        Args:
            interests: ì‚¬ìš©ìžì˜ ê´€ì‹¬ì‚¬
            top_k: ê²€ìƒ‰í•  í•™ê³¼ ê°œìˆ˜

        Returns:
            ì¶”ì²œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # 1. ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ìœ ì‚¬í•œ í•™ê³¼ ì°¾ê¸°
        search_results = self.search_similar_majors(interests, top_k=top_k)

        # 2. LLMì„ ì‚¬ìš©í•˜ì—¬ ì¶”ì²œ ìƒì„±
        recommendation = self.generate_recommendation(interests, search_results)

        return recommendation

    def health_check(self) -> Dict[str, Any]:
        """RAG ì‹œìŠ¤í…œì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            # Qdrant ì—°ê²° í™•ì¸
            from qdrant_client import QdrantClient
            qdrant_client = QdrantClient(url=self.vectorstore._client._host)
            collection_info = qdrant_client.get_collection(self.collection_name)

            # ìž„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸
            test_embedding = self.embeddings.embed_query("í…ŒìŠ¤íŠ¸")

            # LLM í…ŒìŠ¤íŠ¸
            test_response = self.llm.invoke("ì•ˆë…•í•˜ì„¸ìš”")

            return {
                "status": "healthy",
                "vectorstore": "connected",
                "collection_name": self.collection_name,
                "vectors_count": collection_info.points_count,
                "embedding_model": "jhgan/ko-sroberta-multitask",
                "embedding_dim": len(test_embedding),
                "llm_model": self.llm_model,
                "llm_status": "ok" if test_response else "error"
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }


# ì „ì—­ RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_rag_system = None


def get_rag_system() -> MajorRecommendationRAG:
    """ì‹±ê¸€í†¤ RAG ì‹œìŠ¤í…œ ê°€ì ¸ì˜¤ê¸°"""
    global _rag_system
    if _rag_system is None:
        _rag_system = MajorRecommendationRAG()
    return _rag_system
